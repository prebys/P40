\chapter{Monte Carlo Simulation}

\section{Introduction}

In this lab, we will introduce the Monte Carlo method, an approach to
solving a wide range of problems by repeatedly drawing random numbers.
You will use histograms to compare randomly thrown values to their
corresponding probability distributions functions.  You will  measure $\pi$ 
and compute one integral using a Monte Carlo method.

%For a faster but more challenging path, complete only the problems
%(including optional challenge problems) in Sections~\ref{sec:mcint},
%\ref{sec:mbdist}, and \ref{sec:idealgas}.

\section{Preparation}

In this exercise, we will make use of slicing and boolean masks, which
are covered in sections 1.4.1.5 and 1.4.1.7 of the Scientific Python
lecture notes.  Make sure you are comfortable enough with these
concepts to understand the following exercises:\\

\plot Run the following snippet, and make sure you understand the output:
\begin{python}
x = np.arange(10)
print("x       = ", x)
# all but the first element:
print("x[1:]   = ", x[1:])
# all but the last element:
print("x[:-1]  = ", x[:-1])
# the first 4 elements:
print("x[:4]   = ", x[:4])
# elements from index 2 to 6:
print("x[2:7]  = ", x[2:7])
# elements in reverse order:
print("x[::-1] = ", x[::-1])
\end{python} \vskip 0.25cm

\newpage

\plot Run the following snippet, 
\begin{python}
x = np.arange(5)
print("x =         ", x)
print("midpoints:  ", (x[1:]+x[:-1])/2)
\end{python}
which demonstrates a trick we will be using later, for calculating the
midpoints between each value in \pyth{x}.  Note that the first array
has five entries, the second has four.\\

\plot Run the following snippet, and make sure you understand the output:
\begin{python}
x = np.array([1,2,5,1,5,1,8,2])
print("x       = ", x)
mask = x > 2
print("mask    = ", mask)
print("x[mask] = ", x[mask])
\end{python} \vskip 0.25cm

\plot Run the following snippet, 
\begin{python}
x = np.array([7,2,10,4,2,9,1,3])
mask = x>3
np.sum(mask)
\end{python}
which demonstrates a trick we will using later, for counting the
number of entries in an array which satisfy a particular condition.\\

\section{Generating random numbers}

The Monte Carlo method relies on the generation of random numbers, so
we will start there.  The numbers we generate using computers are
actually ``pseudorandom'' numbers, because they are deterministically
obtained from an algorithm.  However, the algorithm is choosen so that
the numbers appear random for practical purposes.  This is no small
concern.  Much of the computational work in the early 1970's had to be
redone because of the widespread use of a deeply flawed pseudorandom
number generator called RANDU.

In this section, you will generate a pseudorandom number sequence
using the linear congruential method.  This sequence is determined
iteratively from the simple relationship:
\begin{displaymath}
  I_{n+1} = (a \, I_{n} + c) \bmod M
\end{displaymath}
Recall that $x \bmod y$ (coded as \pyth{x % y} in Python) is the
remainder after integer division (\pyth{x//y} in Python).  Each $I_n$
is called a seed, and the initial seed $I_0$ must be provided to start
the sequence.  Notice that the seeds are all integers in the range
from 0 to $(M-1)$.  If we wish to convert these seeds into a random
variable $x$ in the range from 0 to $L$, we simply use $x_n = L * I_n
/ M$.  As long as $M$ is much larger than $L$, $x$ is approximately
continuous.

The algorithm works because the product $a*I_{n}$ is generally many
times larger than $M$, so the remainder is effectively a uniform
random number.  The effectiveness of this algorithm is highly
dependend on the choice of $a$,$c$, and $M$.  Choose poorly and you get
RANDU.  Choose wisely and you get the highly regarded algorithm of
Park and Miller.  We will do the latter and use $a=7^5$, $c=0$, and $M
= 2^{31}-1$.\\

\plot Define a function:
\begin{python}
  def parkmiller(i):
      # your code here
      return i # updated value
\end{python}
which, given a seed i, returns the next seed in the Park and Miller
algorithm.  Check your code by testing that for a initial seed of one,
the generator returns a {\bf seed} of 1043618065 after 10000 calls.\\

\plot Use your \pyth{parkmiller(i)} function to fill a numpy array
\pyth{xarr} with 5 randomly thrown $x$ values in the range $[0,1]$.  Check you
code with:
\begin{python}
  print(np.around(xarr,2))
\end{python} \vskip 0.25cm

Now that we have seen how randomly number are generated, we will use the standard numpy tool to produce array of randomly drawn values as needed for us:\\

\plot Use the \pyth{np.random.uniform} to create an array \pyth{xarr}
of five randomly thrown $x$ values in the range $[0,1]$.  Check you
code with:
\begin{python}
  print(np.around(xarr,2))
\end{python}

\section{Probability Density Functions and Histograms}

\begin{figure}[htbp]
\begin{center}
{\includegraphics[width=0.45\textwidth]{figs/hist/flatpdf.pdf}}
\end{center}
\caption{\label{fig:flatpdf} The PDF for the process of throwing a
  random variable uniformly in the region $[0,1]$.}
\end{figure}

\noindent
When I generate ten random numbers from the Park and Miller algorithm, I get
the following ten values:
\begin{verbatim}
[0.6448101  0.32334154 0.40125851 0.95175061 0.07252577 0.94062374
 0.06316977 0.69433175 0.63370098 0.61235022]
\end{verbatim}
We say that these ten numbers are thrown (like dice) or drawn (like
cards?) uniformly in the interval from $[0,1]$.  How can we describe
this process of thowing random numbers in terms of probability?  The
probability of drawing a particular number, like 0.06316977 is
extremely small.  If the computer had unlimited precision, the
probability of drawing any particular number would drop all the way to
zero.  Probability by itself doesn't seem to be very useful here.  The
solution is to recognize that it is the probability of throwing a number in
some region $[a,b]$ which is non-zero.  Instead of a probality, we describe
this process by a probability density function (PDF), in this case:
\begin{equation}
  \label{eqn:flatpdf}
  p(x) =
  \begin{cases}
    1 & 0 \leq x \leq 1 \\
    0 & {\rm otherwise} \\
  \end{cases} 
\end{equation}
This PDF is illustrated in Fig.~\ref{fig:flatpdf}.  To find the probability $P$ that we would throw $x$ in some interval $[a,b]$, we integrate the probability density function:
\begin{displaymath}
  P = \int_a^b p(x) \, dx
\end{displaymath}
For example, the probability that we throw a number less than one half is:
\begin{displaymath}
  P = \int_{-\infty}^\frac{1}{2} p(x) \, dx = \int_{0}^\frac{1}{2} 1 \, dx = \frac{1}{2} 
\end{displaymath}
As are all PDFs, this one is normalized to a total probability of one:
\begin{displaymath}
  \int_{-\infty}^{+\infty} p(x) \, dx = \int_0^1 1 \, dx = 1
\end{displaymath}\\

\plot Define a python function:
\begin{python}
  def pdf(x):
      # your code here
      return p
\end{python}
which implements the PDF from Equation~\ref{eqn:flatpdf}.  Use it to create $x$ and $y$ values for plotting as:
\begin{python}
  xf = np.linspace(-1.5,1.5,100)
  yf = pdf(xf)
\end{python}
Reproduce the plot in Fig.~\ref{fig:flatpdf}.  Hint: for this to work,
you have to make sure the function \pyth{pdf(x)} can handle properly
the case that x is a numpy array, and return a numpy array of the same
size.\\

But how can we be verify that our numbers drawn from the Park and Miller algorithm:
\begin{verbatim}
[0.6448101  0.32334154 0.40125851 0.95175061 0.07252577 0.94062374
 0.06316977 0.69433175 0.63370098 0.61235022]
\end{verbatim}
are actually being drawn from the PDF in Fig.~\ref{fig:flatpdf}?  The
tool of choice for seeing the ``shape'' of a list of values is the
histogram, and the process of building one is illustruated in
Fig.~\ref{fig:histeg}.  First, let's draw 1000 random values.  One way
to visualize these values is shown in Fig.~\ref{fig:histeg}a, which
simply plots each value above the throw number (from 0 to 1000).  To
build a histogram, we devide the $x$ range into small regions, called
{\em bins}.  For example, we have a bin from 0.25 to 0.5, as indicated
by the dashed red lines.  The number of blue points contained within
that range is 237.  In Fig.~\ref{fig:histeg}b, we plot the count as
the red point.  The $y$-value of the point is the count 237.  The
$x$-value is the middle of the bin:
\begin{displaymath}
(0.50+0.25)/2 = 0.375
\end{displaymath}
We continue this process for as many bins as we wish to plot.  The
result is the entire set of points in Fig.~\ref{fig:histeg}b which we
call a histogram.  A histogram is a set of bins, with a count
corresponding to each bin.

\begin{figure}[htbp]
\begin{center}
\begin{tabular}{cc}
{\includegraphics[width=0.45\textwidth]{figs/hist/throwflat.pdf}} &
{\includegraphics[width=0.45\textwidth]{figs/hist/flathist.pdf}} \\
(a) & (b) \\
\end{tabular}
\end{center}
\caption{\label{fig:histeg} The 1000 measurements of variable $x$ in (a) are used to produce the histogram in (b).  The red data point in (b) is the count of the number of entries in range indicated by the red dashed lines in (a).}
\end{figure}

Notice that each data point in our histogram has an error bar: the
verticle line passing through each point in the histogram.  A
fundamental result from statistics is that the best estimate for the
statistical uncertainty on a count $N$ of independent occurances is
simply $\sqrt{N}$.  The error bars in Fig.~\ref{fig:histeg}b have been
taken as the square root of the count in each bin.

Fortunately, the process of calculating a histogram from an array of values is handled by the python function \pyth{np.histogram}, which you will use in the following exercise:\\

\plot Run the following code snippet:
\begin{python}
NTOT = 1000  # total number of events thrown
NBIN = 12    # number of bins in histogram
XMIN = -1.5  # maximum X value
XMAX = 1.5   # minimum X value
# throw NTOT random values uniformly in [0,1]:
xarr = np.random.uniform(size=NTOT)
#create a histogram from the random values in xarr:
# hx:   the histogram counts (length NBIN)
# edges: the bin edges (length NBIN+1)
hx,edges = np.histogram(xarr,bins=NBIN,range=(XMIN,XMAX))
# calculate the center of each bin, for plotting:
cbins = (edges[1:]+edges[:-1])/2
# calculate the error in each bin as the square root of the count
err   = hx**0.5
# plot the histogram, including errorbars, using the errorbar function:
plt.errorbar(cbins,hx,yerr=err,fmt="ko",label="Histogram")
# add the labels
plt.xlabel("x")
plt.ylabel("Entries")
\end{python}
to construct a histogram like that of Fig.~\ref{fig:histeg}b.\\

There are some lines of particular importance in the code snippet,
which you will need to understand to succeed in this lab.  This line:
\begin{python}
hx,edges = np.histogram(xarr,bins=NBIN,range=(XMIN,XMAX))
\end{python}
actually creates the histogram for the array \pyth{xarr}.  It creates
\pyth{NBIN=12} bins in range from \pyth{XMIN=-1.5} to \pyth{XMAX=1.5}.
The count for each bin is contained in the array \pyth{hx} which has length \pyth{NBIN}.  The edges of the bins are contained in the array \pyth{bins} which has length \pyth{NBIN+1}. Because they are different lengths, you cannot simply plot \pyth{hx} versus \pyth{bins}.  Instead, we calculate the bin centers with the line:
\begin{python}
cbins = (edges[1:]+edges[:-1])/2
\end{python}
which uses slicing to produce an array of length \pyth{NBIN}
containing the bin centers.
We calculate the statistical error for each point in the histogram as the square root of the count:
\begin{python}
err   = hx**0.5
\end{python}
The line:
\begin{python}
plt.errorbar(cbins,hx,yerr=err,fmt="ko",label="generated")
\end{python}
plots the histogram with errorbars.  Sometimes students are confused
by the name of the \pyth{plt.errorbar} function: it plots both the
histogram and the errorbars!\\

\plot Modify the code snippet from the previous example to create and
draw a histogram from 1000 random values thrown in the range $[0,1]$
using your Park and Miller algorithm.\\

Your histogram in the previous exercise should look much like that of
Fig.~\ref{fig:histeg}b.  We can also see that it has the same shape as
the PDF in Fig.~\ref{fig:flatpdf}.  But the histogram has a maximum
value of around 270, whereas the PDF has a maximum value of 1.  This
is because the histogram presents a count and the PDF presents a
probability density.  For $N_{\rm tot}$ total events thrown, we can
use the PDF $p(x)$ to predict the number of events $N_{ab}$ in a bin
with edges $a$ and $b$ as:
\begin{displaymath}
N_{ab} = N_{\rm tot} \int_a^b p(x) \, dx \\
\end{displaymath}
where $p(x)$ is the PDF, $N_{\rm tot}$ are the number of throws.  From
the mean value theorem we can find a particular $x^*$ in the range
$[a,b]$ such that
\begin{displaymath}
\int_a^b p(x) \, dx = (b-a) \, p(x^*)
\end{displaymath}
and so we have:
\begin{displaymath}
N_{ab} = N_{\rm tot} \, \Delta x \cdot p(x^*) \\
\end{displaymath}
where $\Delta x = b-a$ is the bin size.  As a practical matter, 
instead of calculating $N_{ab}$ for each bin, we simply plot the PDF scaled as:
\begin{displaymath}
N(x) = N_{\rm tot} \, \Delta x \cdot p(x)  \\
\end{displaymath}
which allows us to directly compare a PDF to the histogram.  In our case, the scale factor is:
\begin{displaymath}
N_{\rm tot} \, \Delta x = 1000 * (0.50-0.25) = 250.
\end{displaymath}
A comparison of the histogram with the PDF scaled by this factor is shown in Fig.~\ref{fig:histpdf}.\\

\begin{figure}[htbp]
\begin{center}
{\includegraphics[width=0.45\textwidth]{figs/hist/histpdf.pdf}}
\end{center}
\caption{\label{fig:histpdf} Direct comparison of a histogram containing 1000 events thrown uniformly in the range $[0,1]$ with the corresponding PDF, scaled appropriately.}
\end{figure}

\plot Add a plot of the scaled PDF to your histogram, to produce a figure like that of Fig.~\ref{fig:histpdf}\\


\section{Calculating the value of $\pi$}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.65\textwidth]{figs/monte_carlo/pitoss.jpg} 
\caption{Determining $\pi$ by throwing toothpicks.}
\label{fig:pitoss}
\end{center}
\end{figure}

\noindent
You can be the life of your next party by showing off how to determine
the constant $\pi$ by throwing toothpicks!  The procedure is simple:
you cut a piece of paper to a width of four toothpicks, then draw two
vertical lines separated by the width of two tooth picks.  Take turns
tossing toothpicks, as in Fig.~\ref{fig:pitoss}.

From the geometry of the setup, it can be shown that the probability
that a toothpick which is entirely on the paper also crosses a line is
given by $1/\pi$.  Therefore, one can measure $\pi$ by counting the
total number of toothpicks that landed entirely on the page and
dividing by the number of those toothpicks that crossed a line.  This
is, in essence, the Monte Carlo method.

\begin{figure}[htbp]
\begin{center}
  \includegraphics[width=0.9\textwidth]{figs/ideal_gas/pimc.pdf}
\caption{Monte Carlo Determination $\pi$ .}
\label{fig:pimc}
\end{center}
\end{figure}

An easier Monte Carlo method to implement computationally is 
shown in Fig.~\ref{fig:pimc} which was generated with following code:
\begin{python}
N = 1000 # number of random values to throw
# throw N x and y random variables uniform in [0,1]
x = np.random.uniform(size = N)
y = np.random.uniform(size = N)
# determine which (x,y) position or inside/outside the unit circle:
rsq = x**2 + y**2
inside  = rsq<=1
outside = np.logical_not(inside)
# set aspect ratio to 1 so unit circle looks like a circle.
plt.axes().set_aspect('equal')
# plot inside as blue dots and outside as red dots
plt.plot(x[inside],y[inside],"b.",label="inside")
plt.plot(x[outside],y[outside],"r.",label="outside")
# plot the unit circle:
xfin = np.linspace(0,1,100)
yfin = sqrt(1-xfin**2)
plt.plot(xfin,yfin,"k-",label="$x^2+y^2=1$")
# add labels and legends:
plt.xlabel("x")
plt.ylabel("y")
plt.legend(loc=3)
\end{python}
The idea is to throw points uniformly in the unit square of area 1.
Much like in the toothpick example, the value of $\pi$ can be
determined by counting the number of generated points which also
landed within the unit circle.  Make sure you understand the example
code, particular how masks are used to draw the blue and red dots.\\

\plot Run the example code.  Then, count the number of points inside the unit circle using:
\begin{python}
n_inside = np.sum(inside)
\end{python}
Estimate $\pi$ using the Monte Carlo method.  Hint: based on area, what fraction of total events do you expect to find within the unit circle?\\

\plot  This is an example of a binomial process, and the statistical
uncertainty on your measured value of $\pi$ works out to be:
\begin{displaymath}
\sigma_\pi = \sqrt{\frac{\pi \, (4-\pi)}{n}}
\end{displaymath}
where $n$ is the number of generated events.  Does your measured value
of $\pi$ agree with the known value within your statistical
uncertainty?\\

\section{Monte Carlo integration}
\label{sec:mcint}

The Monte Carlo method can also be used to numerically integrate a
function.  Monte Carlo integration methods generally only outperform
deterministic methods when the number of dimensions is large, but we
can illustrate the method most easily in one dimension. In this
section, you'll use the Monte Carlo method to perform the integral:
\begin{displaymath}
  \int_0^\pi \sin^2 \theta \, d\theta
\end{displaymath}

To do so, you should make a copy of your solution from the previous section
and modify it in the following manner:
\begin{itemize}
 \item Instead of thowing $x$ in $[0,1]$, throw $\theta$ in $[0,\pi]$.  This means the area of the rectangle $A$ is now $\pi$ instead of 1.
 \item Count the number of throws that land below the integral $y < \sin^2 \theta$.
 \item Determine the area under the curve as the fraction of the throws under the curve times the total area of the rectangle $A$.
 \item The statistical uncertainty in this case is $\pi/(2\sqrt{n})$ where $n$ is the number of generated events.
\end{itemize}  

\begin{plot} \end{plot}
Use the Monte Carlo method to calculate the integral:
\begin{displaymath}
  \int_0^\pi \sin^2 \theta \, d\theta
\end{displaymath}
Make a plot similar to that of Fig.~\ref{fig:pimc} showing the thows
above the curve in red and below the curve in blue.  Calculate the
integral and statistical uncertainty and compare it to the value you obtain analytically.


